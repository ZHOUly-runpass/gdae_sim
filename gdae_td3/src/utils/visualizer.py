"""
TD3 ç®—æ³•å®æ—¶å¯è§†åŒ–å™¨
æ”¯æŒå®æ—¶æ˜¾ç¤ºæœºå™¨äººå¯¼èˆªã€é¿éšœè¡Œä¸ºã€æ¿€å…‰é›·è¾¾æ•°æ®ç­‰
"""
import matplotlib
matplotlib.use('TkAgg')  # ä½¿ç”¨ TkAgg åç«¯ï¼ˆäº¤äº’å¼çª—å£ï¼‰

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Circle, Rectangle
from collections import deque


class TD3Visualizer:
    """
    TD3 å®æ—¶å¯è§†åŒ–å™¨
    æ˜¾ç¤ºï¼šç¯å¢ƒã€æœºå™¨äººã€éšœç¢ç‰©ã€æ¿€å…‰é›·è¾¾ã€è½¨è¿¹ã€çŠ¶æ€ä¿¡æ¯
    """

    def __init__(self, env, agent, figsize=(18, 10), update_interval=50):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨

        Args:
            env: ä»¿çœŸç¯å¢ƒ
            agent: TD3 æ™ºèƒ½ä½“
            figsize: å›¾å½¢å°ºå¯¸
            update_interval: æ›´æ–°é—´éš”ï¼ˆæ¯«ç§’ï¼‰
        """
        self. env = env
        self.agent = agent
        self.update_interval = update_interval

        # åˆ›å»ºå›¾å½¢
        self.fig = plt.figure(figsize=figsize)
        self.fig.suptitle('TD3 Robot Navigation - Real-time Visualization',
                         fontsize=16, fontweight='bold')

        # åˆ›å»ºå­å›¾å¸ƒå±€
        gs = self.fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

        # ä¸»ç¯å¢ƒè§†å›¾ï¼ˆå·¦ä¾§å¤§å›¾ï¼‰
        self.ax_env = self.fig.add_subplot(gs[:, :2])

        # æ¿€å…‰é›·è¾¾è§†å›¾ï¼ˆå³ä¸Šï¼‰
        self.ax_laser = self.fig.add_subplot(gs[0, 2], projection='polar')

        # Qå€¼å’ŒåŠ¨ä½œè§†å›¾ï¼ˆå³ä¸­ï¼‰
        self.ax_action = self.fig.add_subplot(gs[1, 2])

        # å¥–åŠ±æ›²çº¿è§†å›¾ï¼ˆå³ä¸‹ï¼‰
        self.ax_reward = self.fig.add_subplot(gs[2, 2])

        # æ•°æ®è®°å½•
        self.trajectory = []
        self.reward_history = deque(maxlen=200)
        self.step_count = 0
        self.episode_count = 0
        self.total_reward = 0

        # å†å²è½¨è¿¹ï¼ˆç”¨äºæ˜¾ç¤ºå¤šä¸ªepisodeï¼‰
        self.all_trajectories = []

        # åŠ¨ä½œå†å²
        self.action_history = deque(maxlen=50)

        # åˆå§‹åŒ–å›¾å½¢å…ƒç´ 
        self._init_plots()

    def _init_plots(self):
        """åˆå§‹åŒ–æ‰€æœ‰å­å›¾"""
        # ç¯å¢ƒè§†å›¾è®¾ç½®
        self.ax_env.set_xlim(-self.env.map_size/2 - 1, self.env.map_size/2 + 1)
        self.ax_env.set_ylim(-self. env.map_size/2 - 1, self.env. map_size/2 + 1)
        self.ax_env.set_aspect('equal')
        self.ax_env. set_xlabel('X (m)', fontsize=10)
        self.ax_env.set_ylabel('Y (m)', fontsize=10)
        self.ax_env.set_title('Navigation Environment', fontsize=12, fontweight='bold')
        self.ax_env.grid(True, alpha=0.3, linestyle='--')

        # æ¿€å…‰é›·è¾¾è§†å›¾è®¾ç½®
        self. ax_laser.set_ylim(0, self.env. laser_range)
        self. ax_laser.set_title('Lidar Scan', fontsize=10, fontweight='bold', pad=20)
        self. ax_laser.grid(True, alpha=0.3)

        # åŠ¨ä½œè§†å›¾è®¾ç½®
        self.ax_action.set_ylim(-1.2, 1.2)
        self.ax_action.set_xlim(0, 50)
        self.ax_action.set_xlabel('Time Steps', fontsize=9)
        self.ax_action.set_ylabel('Action Value', fontsize=9)
        self.ax_action.set_title('Action History', fontsize=10, fontweight='bold')
        self.ax_action.grid(True, alpha=0.3)
        self.ax_action.axhline(y=0, color='k', linestyle='-', linewidth=0.5)

        # å¥–åŠ±æ›²çº¿è®¾ç½®
        self.ax_reward.set_xlim(0, 200)
        self.ax_reward.set_xlabel('Steps', fontsize=9)
        self.ax_reward.set_ylabel('Reward', fontsize=9)
        self.ax_reward.set_title('Reward History', fontsize=10, fontweight='bold')
        self. ax_reward.grid(True, alpha=0.3)
        self.ax_reward.axhline(y=0, color='k', linestyle='-', linewidth=0.5)

    def _draw_environment(self, obs, action, reward):
        """ç»˜åˆ¶ç¯å¢ƒ"""
        self.ax_env.clear()

        # é‡æ–°è®¾ç½®èŒƒå›´
        self.ax_env.set_xlim(-self. env.map_size/2 - 1, self.env. map_size/2 + 1)
        self.ax_env.set_ylim(-self.env.map_size/2 - 1, self.env.map_size/2 + 1)
        self.ax_env.set_aspect('equal')
        self.ax_env.grid(True, alpha=0.3, linestyle='--')

        # ç»˜åˆ¶åœ°å›¾è¾¹ç•Œ
        border = Rectangle(
            (-self.env.map_size/2, -self.env.map_size/2),
            self.env.map_size, self.env.map_size,
            fill=False, edgecolor='black', linewidth=2
        )
        self.ax_env.add_patch(border)

        # ç»˜åˆ¶éšœç¢ç‰©
        for obs_obj in self.env.obstacles. obstacles:
            obstacle = Circle(
                (obs_obj['x'], obs_obj['y']),
                obs_obj['radius'],
                facecolor='dimgray',
                alpha=0.7,
                edgecolor='black',
                linewidth=1.5,
                zorder=5
            )
            self.ax_env.add_patch(obstacle)

        # ç»˜åˆ¶å†å²è½¨è¿¹ï¼ˆæµ…è‰²ï¼‰
        for traj in self.all_trajectories[-5:]:
            if len(traj) > 1:
                traj_array = np.array(traj)
                self.ax_env. plot(
                    traj_array[:, 0], traj_array[:, 1],
                    color='lightblue', alpha=0.3, linewidth=1, zorder=2
                )

        # ç»˜åˆ¶å½“å‰è½¨è¿¹
        if len(self.trajectory) > 1:
            traj = np.array(self.trajectory)
            self.ax_env.plot(
                traj[:, 0], traj[:, 1],
                color='red', alpha=0.8, linewidth=2.5,
                label='Current Path', zorder=8
            )

            # ç»˜åˆ¶èµ·ç‚¹
            self.ax_env.plot(
                traj[0, 0], traj[0, 1],
                marker='o', markersize=10, color='orange',
                markeredgecolor='black', markeredgewidth=1.5,
                label='Start', zorder=9
            )

        # ç»˜åˆ¶ç›®æ ‡ç‚¹ï¼ˆå¸¦å…‰æ™•æ•ˆæœï¼‰
        goal_glow = Circle(
            (self.env.goal_x, self.env.goal_y),
            0.35,
            color='lime',
            alpha=0.3,
            zorder=7
        )
        self.ax_env.add_patch(goal_glow)

        goal = Circle(
            (self.env.goal_x, self.env.goal_y),
            0.2,
            # color='green',
            facecolor='green',
            alpha=0.9,
            edgecolor='darkgreen',
            linewidth=2,
            label='Goal',
            zorder=10
        )
        self.ax_env.add_patch(goal)

        # ç»˜åˆ¶æœºå™¨äººï¼ˆå¸¦æœå‘ç®­å¤´ï¼‰
        robot_body = Circle(
            (self. env.x, self.env.y),
            0.25,
            # color='dodgerblue',
            facecolor='dodgerblue',
            alpha=0.9,
            edgecolor='darkblue',
            linewidth=2,
            zorder=15
        )
        self.ax_env.add_patch(robot_body)

        # æœºå™¨äººæœå‘ç®­å¤´
        arrow_length = 0.4
        dx = arrow_length * np.cos(self.env.theta)
        dy = arrow_length * np.sin(self.env.theta)
        self.ax_env.arrow(
            self.env.x, self.env.y, dx, dy,
            head_width=0.2, head_length=0.15,
            fc='yellow', ec='orange', linewidth=2,
            zorder=16
        )

        # ç»˜åˆ¶æ¿€å…‰æ‰«æçº¿ï¼ˆéƒ¨åˆ†ï¼‰
        laser_data = obs['laser']
        angles = np.linspace(-np.pi/2, np. pi/2, len(laser_data))

        # åªç»˜åˆ¶æ¯5ä¸ªæ¿€å…‰æŸ
        for i in range(0, len(laser_data), 5):
            if laser_data[i] < self.env.laser_range:
                angle = self.env.theta + angles[i]
                end_x = self.env.x + laser_data[i] * np.cos(angle)
                end_y = self.env. y + laser_data[i] * np.sin(angle)

                # æ ¹æ®è·ç¦»è®¾ç½®é¢œè‰²
                if laser_data[i] < 0.5:
                    color = 'red'
                    alpha = 0.6
                elif laser_data[i] < 1.0:
                    color = 'orange'
                    alpha = 0.4
                else:
                    color = 'cyan'
                    alpha = 0.2

                self. ax_env.plot(
                    [self.env. x, end_x], [self.env.y, end_y],
                    color=color, alpha=alpha, linewidth=0.5, zorder=3
                )

        # ç»˜åˆ¶åˆ°ç›®æ ‡çš„è¿çº¿
        self.ax_env.plot(
            [self.env.x, self.env.goal_x],
            [self.env. y, self.env.goal_y],
            'g--', alpha=0.4, linewidth=1.5, zorder=4
        )

        # æ·»åŠ ä¿¡æ¯æ–‡æœ¬æ¡†
        distance = obs['robot_state'][0]
        angle_to_goal = np.degrees(obs['robot_state'][1])

        info_text = (
            f'Episode: {self.episode_count}\n'
            f'Step: {self.step_count}\n'
            f'Distance: {distance:.2f}m\n'
            f'Angle: {angle_to_goal:.1f}\n'
            f'Reward: {reward:.2f}\n'
            f'Total: {self.total_reward:.1f}'
        )

        # åˆ›å»ºæ–‡æœ¬æ¡†
        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
        self.ax_env.text(
            0.02, 0.98, info_text,
            transform=self.ax_env.transAxes,
            fontsize=10,
            verticalalignment='top',
            bbox=props,
            fontfamily='monospace'
        )

        # æ·»åŠ å›¾ä¾‹
        self.ax_env.legend(loc='upper right', fontsize=9, framealpha=0.9)

        # è®¾ç½®æ ‡é¢˜
        status = "ğŸŸ¢ Active" if distance > 0.3 else "ğŸ¯ Goal Reached!"
        self.ax_env. set_title(
            f'Navigation Environment - {status}',
            fontsize=12, fontweight='bold'
        )

    def _draw_lidar(self, obs):
        """ç»˜åˆ¶æ¿€å…‰é›·è¾¾æ•°æ®ï¼ˆæåæ ‡ï¼‰"""
        self.ax_laser.clear()

        laser_data = obs['laser']
        angles = np.linspace(-np.pi/2, np. pi/2, len(laser_data))

        # ç»˜åˆ¶æ¿€å…‰æ•°æ®
        self.ax_laser.plot(angles, laser_data, 'b-', linewidth=2, label='Scan')
        self.ax_laser.fill(angles, laser_data, 'blue', alpha=0.3)

        # æ ‡è®°å±é™©åŒºåŸŸ
        danger_threshold = 0.6
        danger_indices = np.where(np.array(laser_data) < danger_threshold)[0]
        if len(danger_indices) > 0:
            danger_angles = angles[danger_indices]
            danger_dists = np.array(laser_data)[danger_indices]
            self.ax_laser.scatter(
                danger_angles, danger_dists,
                color='red', s=50, zorder=10,
                label='Danger Zone', alpha=0.8
            )

        self.ax_laser.set_ylim(0, self.env. laser_range)
        self. ax_laser.set_title('Lidar Scan', fontsize=10, fontweight='bold', pad=20)
        self.ax_laser.legend(loc='upper right', fontsize=8)
        self.ax_laser.grid(True, alpha=0.3)

    # åœ¨ _draw_action_history æ–¹æ³•ä¸­
    def _draw_action_history(self, action):
        """ç»˜åˆ¶åŠ¨ä½œå†å²"""
        self.ax_action.clear()

        self.action_history.append(action)

        if len(self.action_history) > 1:
            history = np.array(list(self.action_history))
            steps = np.arange(len(history))

            # ç»˜åˆ¶çº¿é€Ÿåº¦
            self.ax_action.plot(
                steps, history[:, 0],
                'b-', linewidth=2, label='Linear Vel', marker='o', markersize=3
            )

            # ç»˜åˆ¶è§’é€Ÿåº¦
            self.ax_action.plot(
                steps, history[:, 1],
                'r-', linewidth=2, label='Angular Vel', marker='s', markersize=3
            )

            # å¡«å……åŒºåŸŸ
            self.ax_action.fill_between(
                steps, history[:, 0], alpha=0.3, color='blue'
            )
            self.ax_action.fill_between(
                steps, history[:, 1], alpha=0.3, color='red'
            )

            # åªåœ¨æœ‰æ•°æ®æ—¶æ·»åŠ å›¾ä¾‹
            self.ax_action.legend(loc='upper right', fontsize=8)  # â† ç§»åˆ°è¿™é‡Œ

        self.ax_action.set_ylim(-1.2, 1.2)
        self.ax_action.set_xlim(0, 50)
        self.ax_action.axhline(y=0, color='k', linestyle='-', linewidth=0.5)
        self.ax_action.set_xlabel('Time Steps', fontsize=9)
        self.ax_action.set_ylabel('Action Value', fontsize=9)
        self.ax_action.set_title('Action History', fontsize=10, fontweight='bold')
        # self.ax_action.legend(loc='upper right', fontsize=8)  # â† åˆ é™¤è¿™è¡Œ
        self.ax_action.grid(True, alpha=0.3)

    def _draw_reward_history(self, reward):  # â† æ·»åŠ è¿™ä¸ªæ–¹æ³•
        """ç»˜åˆ¶å¥–åŠ±å†å²"""
        self.ax_reward.clear()

        self.reward_history.append(reward)

        if len(self.reward_history) > 1:
            rewards = np.array(list(self.reward_history))
            steps = np.arange(len(rewards))

            # ç»˜åˆ¶å¥–åŠ±æ›²çº¿
            self.ax_reward.plot(
                steps, rewards,
                'g-', linewidth=2, alpha=0.8
            )

            # å¡«å……æ­£å¥–åŠ±ï¼ˆç»¿è‰²ï¼‰å’Œè´Ÿå¥–åŠ±ï¼ˆçº¢è‰²ï¼‰
            self.ax_reward.fill_between(
                steps, 0, rewards,
                where=(rewards >= 0),
                color='green', alpha=0.3, label='Positive'
            )
            self.ax_reward.fill_between(
                steps, 0, rewards,
                where=(rewards < 0),
                color='red', alpha=0.3, label='Negative'
            )

            # ç»˜åˆ¶ç§»åŠ¨å¹³å‡
            if len(rewards) >= 10:
                window = 10
                moving_avg = np.convolve(
                    rewards, np.ones(window) / window, mode='valid'
                )
                avg_steps = steps[window - 1:]
                self.ax_reward.plot(
                    avg_steps, moving_avg,
                    'k--', linewidth=2, label='Moving Avg', alpha=0.7
                )

            # åªåœ¨æœ‰æ•°æ®æ—¶æ·»åŠ å›¾ä¾‹
            self.ax_reward.legend(loc='upper right', fontsize=8)

        # è®¾ç½®åæ ‡è½´å’Œæ ‡ç­¾
        self.ax_reward.set_xlim(0, 200)
        self.ax_reward.axhline(y=0, color='k', linestyle='-', linewidth=0.5)
        self.ax_reward.set_xlabel('Steps', fontsize=9)
        self.ax_reward.set_ylabel('Reward', fontsize=9)
        self.ax_reward.set_title('Reward History', fontsize=10, fontweight='bold')
        self.ax_reward.grid(True, alpha=0.3)

    def reset(self):
        """é‡ç½®å¯è§†åŒ–å™¨"""
        if len(self.trajectory) > 0:
            self.all_trajectories.append(self. trajectory.copy())

        self.trajectory = []
        self. reward_history. clear()
        self.action_history.clear()
        self.step_count = 0
        self.total_reward = 0
        self.episode_count += 1

    def update(self, obs, action, reward):
        """
        æ›´æ–°å¯è§†åŒ–

        Args:
            obs: ç¯å¢ƒè§‚æµ‹
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
        """
        # è®°å½•è½¨è¿¹
        self.trajectory.append([self. env.x, self.env. y])

        # æ›´æ–°ç»Ÿè®¡
        self. step_count += 1
        self.total_reward += reward

        # ç»˜åˆ¶æ‰€æœ‰å­å›¾
        self._draw_environment(obs, action, reward)
        self._draw_lidar(obs)
        self._draw_action_history(action)
        self._draw_reward_history(reward)

        # åˆ·æ–°æ˜¾ç¤º
        plt.pause(0.001)

    def show(self):
        """æ˜¾ç¤ºå›¾å½¢"""
        plt.show()

    def save_figure(self, filename='td3_visualization. png'):
        """ä¿å­˜å½“å‰å›¾å½¢"""
        self. fig.savefig(filename, dpi=150, bbox_inches='tight')
        print(f"å›¾å½¢å·²ä¿å­˜è‡³: {filename}")


# ============================================================
# æµ‹è¯•ä»£ç ï¼ˆå¿…é¡»åœ¨ç±»å®šä¹‰ä¹‹å¤–ï¼‰
# ============================================================
if __name__ == "__main__":
    import sys
    import os
    sys.path.insert(0, os.path.abspath(os. path.join(os.path.dirname(__file__), '.. ', '..')))

    from gdae_td3.src.environment.simulator import RobotSimulator
    from gdae_td3.src.td3.agent import TD3Agent
    import torch

    print("=" * 60)
    print("æµ‹è¯• TD3Visualizer")
    print("=" * 60)

    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    print("\nåˆ›å»ºç¯å¢ƒ...")
    env = RobotSimulator()

    print("åˆ›å»ºæ™ºèƒ½ä½“...")
    agent = TD3Agent(device=torch.device('cpu'))

    # åˆ›å»ºå¯è§†åŒ–å™¨
    print("åˆ›å»ºå¯è§†åŒ–å™¨...")
    visualizer = TD3Visualizer(env, agent)

    # è¿è¡Œæµ‹è¯•
    print("å¼€å§‹æµ‹è¯•å¯¼èˆª.. .\n")
    obs = env.reset()

    for step in range(100):
        # ç®€å•çš„å¯å‘å¼åŠ¨ä½œï¼ˆæœå‘ç›®æ ‡ï¼‰
        distance, angle = obs['robot_state']
        min_laser = min(obs['laser'])

        # é¿éšœé€»è¾‘
        if min_laser < 0.5:
            # è½¬å‘æ›´ç©ºæ—·çš„æ–¹å‘
            left_avg = np.mean(obs['laser'][:len(obs['laser'])//2])
            right_avg = np.mean(obs['laser'][len(obs['laser'])//2:])
            action = [0.1, 0.8 if left_avg > right_avg else -0.8]
        else:
            # æœå‘ç›®æ ‡
            linear_vel = min(0.4, distance / 2.0)
            angular_vel = np.clip(angle * 2.0, -0.5, 0.5)
            action = [linear_vel, angular_vel]

        action_in = [(action[0] + 1) / 2, action[1]]
        next_obs, reward, done, info = env.step(action_in)

        # æ›´æ–°å¯è§†åŒ–
        visualizer.update(obs, action, reward)

        if done:
            print(f"\nEpisode ç»“æŸäºç¬¬ {step+1} æ­¥")
            if info.get('distance_to_goal', 1.0) < 0.3:
                print("âœ“ æˆåŠŸåˆ°è¾¾ç›®æ ‡ï¼")
            elif info.get('collision', False):
                print("âœ— å‘ç”Ÿç¢°æ’")
            break

        obs = next_obs

    print("\næµ‹è¯•å®Œæˆï¼å…³é—­çª—å£ä»¥é€€å‡º...")
    visualizer.show()